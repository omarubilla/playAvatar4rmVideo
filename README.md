# playAvatar4rmVideo
![](https://img.shields.io/static/v1?label=python&message=3.6|3.7&color=blue)
![](https://img.shields.io/static/v1?label=pytorch&message=1.4&color=<COLOR>)
[![](https://img.shields.io/static/v1?label=license&message=BSD3&color=green)](./License.txt)

Turn sports practice film into playable avatars in 3D gaming environment

## Step 1: Import MP4 File

## Step 2: Map Scene to 3D Environment

## Step 3: Add Playable Avatar 

## Step 4: Controls: Zoom/Pan/Switch Player/Next Play 



# Acknowledgements

This work was created with guidance from Jiaman Li and Yifeng Jiang from Stanford's Movement Lab.

We are grateful for the guidance of proffesor Karen Liu, along with the greater Wu Tsai Human Performance Alliance community at Stanford University.

Demo videos and prototyping was first conducted at De Anza College in Cupertino, CA. [Apple Founders-De Anza Alumni](https://apple.fandom.com/wiki/De_Anza_College)

# References
This repository is built on top of the following repositories:
* Learning Physically Simulated Tennis Skills from Broadcast Videos [Hoatian Zhang](https://github.com/nv-tlabs/vid2player3d)
* Controllable Character Video Synthesis with Spatial Decomposed Modeling [MIMO](https://github.com/menyifang/MIMO)
    
## Contact
Kevin Ubilla kevin@elisxr.com
